{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping PV data\n",
    "----------\n",
    "In this file, I am scraping the link http://nemweb.com.au/Reports/Archive/ROOFTOP_PV/ACTUAL/ from the data it has.\n",
    "\n",
    "upon inspection, it contains nested zips that contains more dates to the same zips. (example: file `PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20230223` outputs one CSV when exported manually but outputs 112 zips when extracted programmatically (command line or python)\n",
    "\n",
    "The steps are as follows:\n",
    "- I get the zip urls from the website and store them in a list\n",
    "- iterate over the list and download them into a folder \n",
    "-[ TODO ] iterate over the zips in `unzipped_files` and export them to CSVs\n",
    "- read the CSVs in pandas and cocat into the same dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ZIP URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# The URL of the page you want to scrape\n",
    "url = \"http://nemweb.com.au/Reports/Archive/ROOFTOP_PV/ACTUAL/\"\n",
    "base_url= \"http://nemweb.com.au\"\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    zip_urls=[]\n",
    "    # Find all <a> tags\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    \n",
    "    # Filter out links and print them\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        print(base_url)\n",
    "        if href and href.endswith('.zip'):  # Check if the link ends with .zip\n",
    "            full_url = base_url + href if not href.startswith('http') else href\n",
    "#             print(href)\n",
    "#             print(full_url)\n",
    "            zip_urls.append(full_url)\n",
    "else:\n",
    "    print(f\"Failed to retrieve content from {url}, status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define the folder name where files will be saved\n",
    "folder_name = 'zips'\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "for file_url in zip_urls:\n",
    "    print(file_url)\n",
    "    response = requests.get(file_url)\n",
    "    filename = file_url.split('/')[-1]  # Extracts the filename\n",
    "\n",
    "    # Modify the path to save the file in the new folder\n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "    # Save the file in the specified folder\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded {filename} into {folder_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the source and destination folder paths\n",
    "source_folder = 'zips'\n",
    "destination_folder = 'unzipped_files'\n",
    "\n",
    "# Check if the destination folder exists, if not, create it\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# List all the files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(source_folder, filename)\n",
    "    \n",
    "    # Check if the file is a ZIP file\n",
    "    if zipfile.is_zipfile(file_path):\n",
    "        print(f\"Unzipping {filename}\")\n",
    "        \n",
    "        # Open the ZIP file\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            # Extract all the contents into the destination folder\n",
    "            zip_ref.extractall(destination_folder)\n",
    "        \n",
    "        print(f\"Extracted {filename} into {destination_folder}\")\n",
    "    else:\n",
    "        print(f\"{filename} is not a zip file. Skipping.\")\n",
    "\n",
    "print(\"Unzipping complete.\")\n",
    "\n",
    "# the unzip process isn't working as expected, we have encapsulated zip' files, so I need to do this again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks with the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using my trusty command line\n",
    "#!unzip -o \"zips/PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20230223.zip\" -d \"zips/unzipped/\"\n",
    "# !unzip -o 'zips/unzipped/PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20230223000000_0000000381390815.zip' -d \"zips/unzipped/\"\n",
    "!unzip -o 'zips/unzipped/PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20230223003000_0000000381392042.zip' -d \"zips/unzipped/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test zip file\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to the zip file and the extraction directory\n",
    "zip_path = 'zips/PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20230223.zip'\n",
    "extraction_path = 'zips/unzipped'  # Directory where files will be extracted\n",
    "\n",
    "# Ensure the extraction directory exists\n",
    "if not os.path.exists(extraction_path):\n",
    "    os.makedirs(extraction_path)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    # Extract all the contents into the directory\n",
    "    zip_ref.extractall(extraction_path)\n",
    "    print(f\"Contents of the ZIP file extracted to {extraction_path}\")\n",
    "\n",
    "# List the contents of the extraction directory\n",
    "extracted_files = os.listdir(extraction_path)\n",
    "print(\"Extracted files:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# If you expect only one CSV and it's directly in the ZIP, let's find it\n",
    "csv_files = [file for file in extracted_files if file.endswith('.csv')]\n",
    "if csv_files:\n",
    "    print(\"CSV files found:\")\n",
    "    for csv_file in csv_files:\n",
    "        print(csv_file)\n",
    "else:\n",
    "    print(\"No CSV files found directly in the ZIP file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unzip round 2\n",
    "Extracts a total of 37k csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the source and destination folder paths\n",
    "source_folder = 'unzipped_files'\n",
    "destination_folder = 'unzipped_files_round2'\n",
    "\n",
    "# Check if the destination folder exists, if not, create it\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "i=0\n",
    "# List all the files in the source folder\n",
    "for filename in os.listdir(source_folder):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(source_folder, filename)\n",
    "    \n",
    "    # Check if the file is a ZIP file\n",
    "    if zipfile.is_zipfile(file_path):\n",
    "#         print(f\"Unzipping {filename}\")\n",
    "        \n",
    "        # Open the ZIP file\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            # Extract all the contents into the destination folder\n",
    "            zip_ref.extractall(destination_folder)\n",
    "        \n",
    "        print(f\"Extracted {filename} into {destination_folder}\")\n",
    "        i+=1\n",
    "        if (i%1000==0): # I tried it for the first 100 zips in the second directory and seems to be working\n",
    "            print('extracted ', i, \" Csvs\")\n",
    "    else:\n",
    "        print(f\"{filename} is not a zip file. Skipping.\")\n",
    "\n",
    "print(\"Unzipping complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read as pandas and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory where your CSV files are located\n",
    "directory = 'unzipped_files_round2'\n",
    "\n",
    "# List to hold your DataFrames\n",
    "dataframes_list = []\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(directory, filename)  # Get the full path to the file\n",
    "        df = pd.read_csv(file_path)  # Read the CSV file into a DataFrame\n",
    "        dataframes_list.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>NEMP.WORLD</th>\n",
       "      <th>ROOFTOP_PV_ACTUAL_MEASUREMENT</th>\n",
       "      <th>AEMO</th>\n",
       "      <th>PUBLIC</th>\n",
       "      <th>2023/03/23</th>\n",
       "      <th>06:00:02</th>\n",
       "      <th>0000000383340378</th>\n",
       "      <th>DEMAND</th>\n",
       "      <th>0000000383340378.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>ROOFTOP</td>\n",
       "      <td>ACTUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>INTERVAL_DATETIME</td>\n",
       "      <td>REGIONID</td>\n",
       "      <td>POWER</td>\n",
       "      <td>QI</td>\n",
       "      <td>TYPE</td>\n",
       "      <td>LASTCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>ROOFTOP</td>\n",
       "      <td>ACTUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023/03/23 05:30:00</td>\n",
       "      <td>NSW1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEASUREMENT</td>\n",
       "      <td>2023/03/23 05:49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>ROOFTOP</td>\n",
       "      <td>ACTUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023/03/23 05:30:00</td>\n",
       "      <td>QLD1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEASUREMENT</td>\n",
       "      <td>2023/03/23 05:49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>ROOFTOP</td>\n",
       "      <td>ACTUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023/03/23 05:30:00</td>\n",
       "      <td>QLDC</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEASUREMENT</td>\n",
       "      <td>2023/03/23 05:49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>ROOFTOP</td>\n",
       "      <td>ACTUAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023/03/23 05:30:00</td>\n",
       "      <td>QLDN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEASUREMENT</td>\n",
       "      <td>2023/03/23 05:49:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C NEMP.WORLD ROOFTOP_PV_ACTUAL_MEASUREMENT  AEMO               PUBLIC  \\\n",
       "0  I    ROOFTOP                        ACTUAL   2.0    INTERVAL_DATETIME   \n",
       "1  D    ROOFTOP                        ACTUAL   2.0  2023/03/23 05:30:00   \n",
       "2  D    ROOFTOP                        ACTUAL   2.0  2023/03/23 05:30:00   \n",
       "3  D    ROOFTOP                        ACTUAL   2.0  2023/03/23 05:30:00   \n",
       "4  D    ROOFTOP                        ACTUAL   2.0  2023/03/23 05:30:00   \n",
       "\n",
       "  2023/03/23 06:00:02 0000000383340378       DEMAND   0000000383340378.1  \n",
       "0   REGIONID    POWER               QI         TYPE          LASTCHANGED  \n",
       "1       NSW1        0                1  MEASUREMENT  2023/03/23 05:49:23  \n",
       "2       QLD1        0                1  MEASUREMENT  2023/03/23 05:49:23  \n",
       "3       QLDC        0                1  MEASUREMENT  2023/03/23 05:49:23  \n",
       "4       QLDN        0                1  MEASUREMENT  2023/03/23 05:49:23  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (len(dataframes_list))\n",
    "dataframes_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "# Now you have a single DataFrame 'combined_df' containing all the data\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: debugging to get the zips downloaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interlude: 2 hours of debugging to get the links right\n",
    "\n",
    "# import requests\n",
    "# import os\n",
    "# import hashlib\n",
    "\n",
    "# def verify_file_checksum(file_path, original_checksum):\n",
    "#     \"\"\"Verify file checksum.\"\"\"\n",
    "#     sha256_hash = hashlib.sha256()\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         # Read and update hash in chunks of 4K\n",
    "#         for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "#             sha256_hash.update(byte_block)\n",
    "#     return sha256_hash.hexdigest() == original_checksum\n",
    "\n",
    "# # Define the folder name where files will be saved\n",
    "# folder_name = 'zips'\n",
    "\n",
    "# # Check if the folder exists, if not, create it\n",
    "# if not os.path.exists(folder_name):\n",
    "#     os.makedirs(folder_name)\n",
    "\n",
    "# for file_url in zip_urls:\n",
    "#     print(file_url)\n",
    "#     response = requests.get(file_url, stream=True)  # Stream the download\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         filename = file_url.split('/')[-1]  # Extracts the filename\n",
    "#         file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "#         # Save the file in the specified folder\n",
    "#         with open(file_path, 'wb') as file:\n",
    "#             for chunk in response.iter_content(chunk_size=8192):\n",
    "#                 file.write(chunk)\n",
    "\n",
    "#         print(f\"Downloaded {filename} into {folder_name}\")\n",
    "\n",
    "#         # Example checksum verification (assuming you have the original checksum)\n",
    "#         # original_checksum = 'the_checksum_provided_for_the_file'\n",
    "#         # if verify_file_checksum(file_path, original_checksum):\n",
    "#         #     print(f\"Checksum verified for {filename}\")\n",
    "#         # else:\n",
    "#         #     print(f\"Checksum mismatch for {filename}\")\n",
    "#     else:\n",
    "#         print(f\"Failed to download {filename}. Status code: {response.status_code}\")\n",
    "\n",
    "        \n",
    "# import requests\n",
    "\n",
    "# def download_file(url, filename):\n",
    "#     \"\"\"\n",
    "#     Download a file from a URL and save it to the local filesystem.\n",
    "\n",
    "#     Parameters:\n",
    "#     - url: The URL of the file to download.\n",
    "#     - filename: The local filename to save the downloaded file.\n",
    "#     \"\"\"\n",
    "#     # Send a GET request to the URL\n",
    "#     response = requests.get(url)\n",
    "    \n",
    "#     # Check if the request was successful\n",
    "#     if response.status_code == 200:\n",
    "#         # Open a local file with write-binary ('wb') mode\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "#         print(f\"File downloaded successfully: {filename}\")\n",
    "#     else:\n",
    "#         print(f\"Failed to download file. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "# # Example usage\n",
    "# url = zip_urls[0]\n",
    "# filename = 'local_filename.zip'\n",
    "# download_file(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://nemweb.com.au/Reports/Archive/ROOFTOP_PV/ACTUAL/PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20230223.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
